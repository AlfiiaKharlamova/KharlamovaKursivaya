{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Запустите все ячейки до раздела \"Для пользователя\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "from itertools import groupby\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing_urls(link, search_element):\n",
    "    \n",
    "    \"\"\"Parsing links from news agencies front page\"\"\"\n",
    "    \n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.text)\n",
    "    find = soup.findAll('a') # looking for all links\n",
    "    \n",
    "    urls = []\n",
    "    for l in find:\n",
    "        variable = str(l.get('href')) # links to string\n",
    "        if search_element in variable:\n",
    "            urls.append(l.get('href'))\n",
    "        sleep(0.5) # delay to avoid disconnection\n",
    "    new_urls = [el for el, _ in groupby(urls)] # to remove duplicates\n",
    "    \n",
    "    full_urls = [link + x for x in new_urls]\n",
    "    \n",
    "    j = 0\n",
    "    if 'lenta' in link: # loop to avoid ConnectionError while parsing Lenta headlines and news\n",
    "        # program hasn't ConnectionError while parsing TASS or Izvestia headlines and news\n",
    "        while j < len(full_urls):\n",
    "            if 'lenta.ruhttps' in full_urls[j]:\n",
    "                del full_urls[j]\n",
    "            else:\n",
    "                j = j + 1\n",
    "    \n",
    "    j = 0\n",
    "    if 'kommersant' in link: # loop to avoid ConnectionError while parsing Kommersant headlines and news\n",
    "        while j < len(full_urls):\n",
    "            if 'kommersant.ruhttps' in full_urls[j]:\n",
    "                del full_urls[j]\n",
    "            elif '\\t' in full_urls[j]:\n",
    "                del full_urls[j]\n",
    "            else:\n",
    "                j = j + 1\n",
    "        \n",
    "    return full_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing_heads(urls):\n",
    "    \n",
    "    \"\"\"News headlines parser\"\"\"\n",
    "    \n",
    "    headlines = []\n",
    "    i = 0\n",
    "    while i < len(urls):\n",
    "        p = requests.get(urls[i])\n",
    "        s = BeautifulSoup(p.text)\n",
    "        headline = s.findAll('meta', {'property':'og:title'})\n",
    "        if len(headline) > 0: #to avoid index error for empty lists\n",
    "            headlines.append(headline[0]['content'])\n",
    "        i = i + 1\n",
    "        sleep(0.5)\n",
    "    return headlines\n",
    "\n",
    "def parsing_news(urls, clss):\n",
    "    \n",
    "    \"\"\"Bodies of news parser\"\"\"\n",
    "    \n",
    "    texts = []\n",
    "    i = 0\n",
    "    while i < len(urls):\n",
    "        p = requests.get(urls[i])\n",
    "        s = BeautifulSoup(p.text)\n",
    "        text = s.findAll('p', {'class': clss})\n",
    "        texts.append(text)\n",
    "        i = i + 1\n",
    "        sleep(0.5)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_form(news, title):\n",
    "    \n",
    "    \"\"\"Text formatting for easy visualization\"\"\"\n",
    "    \n",
    "    i = 0\n",
    "    texts = news\n",
    "    texts_new = []\n",
    "    for r in texts:\n",
    "        while i < len(r):\n",
    "            texts_new.append(r[i]) \n",
    "            i = i + 1\n",
    "        i = 0\n",
    "                \n",
    "    final_texts = []\n",
    "    for r in texts_new:\n",
    "        final_texts.append(r.text) # pastes text with the exception of HTML elements\n",
    "    final = ' '.join(final_texts) # everything into one string\n",
    "    joined_heads = ' '.join(title)\n",
    "    final = joined_heads + final\n",
    "    \n",
    "    final = final.replace('МОСКВА', '').strip()\n",
    "    final = final.replace('/ТАСС/.', '')\n",
    "    final = final.replace('\\xa0с', '')\n",
    "    final = final.replace('Авторское право на систему визуализации содержимого портала iz.ru, а также на исходные данные, включая тексты, фотографии, аудио- и видеоматериалы, графические изображения, иные произведения и товарные знаки принадлежит ООО «МИЦ «Известия». Указанная информация охраняется в соответствии с законодательством РФ и международными соглашениями', '')\n",
    "    final = final.replace('Частичное цитирование возможно только при условии гиперссылки на iz.ru', '')\n",
    "    final = final.replace('АО «АБ «РОССИЯ» — партнер рубрики «Экономика»', '')\n",
    "    final = final.replace('Сайт функционирует при финансовой поддержке Федерального агентства по печати и массовым коммуникациям', '')\n",
    "    final = final.replace('Ответственность за содержание любых рекламных материалов, размещенных на портале, несет рекламодатель', '')\n",
    "    final = final.replace('Новости, аналитика, прогнозы и другие материалы, представленные на данном сайте, не являются офертой или рекомендацией к покупке или продаже каких-либо активов', '')\n",
    "    final = final.replace('Зарегистрировано Федеральной службой по надзору в сфере связи, информационных технологий и массовых коммуникаций', '')    \n",
    "    final = final.replace('Свидетельства о регистрации', '')\n",
    "    final = final.replace('Все права защищены © ООО «МИЦ «Известия»', '')\n",
    "    final = final.replace('ЭЛ № ФС', '')\n",
    "    final = final.replace('\\xa0', '')\n",
    "    final = final.replace('\\n', '')\n",
    "        \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tass_urls = parsing_urls('https://tass.ru', '/8')\n",
    "tass_heads = parsing_heads(tass_urls)\n",
    "tass_news = parsing_news(tass_urls, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "izvestia_urls = parsing_urls('https://iz.ru', '/2020')\n",
    "izvestia_heads = parsing_heads(izvestia_urls)\n",
    "izvestia_news = parsing_news(izvestia_urls, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kommersant_urls = parsing_urls('https://www.kommersant.ru', '/4')\n",
    "kommersant_heads = parsing_heads(kommersant_urls)\n",
    "kommersant_news = parsing_news(kommersant_urls, 'b-article__text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenta_urls = parsing_urls('https://lenta.ru', '/news')\n",
    "lenta_heads = parsing_heads(lenta_urls)\n",
    "lenta_news = parsing_news(lenta_urls, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tass_form = text_form(tass_news, tass_heads)\n",
    "izvestia_form = text_form(izvestia_news, izvestia_heads)\n",
    "kommersant_form = text_form(kommersant_news, kommersant_heads)\n",
    "lenta_form = text_form(lenta_news, lenta_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_words(result):\n",
    "    \n",
    "    \"\"\"Getting rid of word cases\"\"\"\n",
    "    \n",
    "    list_of_words = []\n",
    "    split_text = result.split(' ')\n",
    "    for word in split_text: # brings words (except verbs) to initial form\n",
    "        p = morph.parse(word)[0] \n",
    "        if('VERB' in p.tag):\n",
    "            list_of_words.append(word)\n",
    "        elif(~('VERB' in p.tag)):\n",
    "            list_of_words.append(p.normal_form)\n",
    "    norm_text = ' '.join(list_of_words).upper()\n",
    "    return norm_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tass = normalize_words(tass_form)\n",
    "izvestia = normalize_words(izvestia_form)\n",
    "lenta = normalize_words(lenta_form)\n",
    "kommersant = normalize_words(kommersant_form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = tass + izvestia + lenta + kommersant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_cloud(newsag, cloud_title):\n",
    "    \n",
    "    \"\"\"Word cloud generating\"\"\"\n",
    "    \n",
    "    STOPWORDS.update({'в', 'на', 'за', 'c', 'к', 'до', 'для', 'что', 'кроме' 'того', 'также', 'по', 'он', 'она', 'него', 'ему',\n",
    "                 'им', 'о', 'об', 'его', 'так', 'все', 'только', 'как', 'сейчас', 'мы', 'был', 'надо', 'когда',\n",
    "                 'это', 'будет', 'было', 'рф', 'россия', 'из', 'от', 'россии', 'за', 'нет', 'если', 'чтобы', 'сказал',\n",
    "                 'меня', 'их', 'уже', 'или', 'после', 'лет', 'были', 'который', 'где', 'при', 'вы', 'согласно',\n",
    "                 'того', 'этого', 'всегда', 'однако', 'есть', 'очень', 'может', 'меня', 'мне', 'не', 'ее', 'раз',\n",
    "                 'да', 'вот', 'но', 'этом', 'кроме', 'которая', 'была', 'чем', 'там', 'еще', 'тогда', 'они', 'бы',\n",
    "                 'же', 'год', 'году', 'года', 'годом', 'годе', 'ранее', 'потом', 'даже', 'которые', 'то', 'пока', 'через',\n",
    "                 'со', 'ну', 'ли', 'более', 'можно', 'всего', 'тем', 'себя', 'из', 'из-за', 'во', 'будут', 'поэтому',\n",
    "                 'перед', 'том', 'теперь', 'этот', 'кто', 'быть', 'у', 'нас', 'понедельник', 'вторник', 'среда', 'четверг', \n",
    "                 'пятница', 'суббота', 'воскресенье', 'январь', 'февраль', 'март', 'апрель', 'май', 'июнь', 'июль', 'август',\n",
    "                 'сентябрь', 'октябрь', 'ноябрь', 'декабрь', 'хотя', 'этого', 'эти', 'этой', 'этих', 'этим', 'ни', 'весь', \n",
    "                 'тот', 'свой', 'такой', 'какой', 'ещё', 'еще', 'один', 'два', 'человек', 'изз', 'изза', 'наш', 'наши',\n",
    "                 'нашим','нашему', 'нашими', 'нами', 'нам', 'мой', 'моему', 'время', 'страна', 'день', 'слово', 'работа', \n",
    "                 'слово', 'другой', 'ТАСС', 'сообщил', 'отметил', 'дать', 'быть', 'тысяч', 'тысяча', 'заявил', 'ситуация',\n",
    "                 'должный', 'гражданин', 'россиянин', 'мая', 'тоже', 'без', 'тыс'})\n",
    "    stopwords = STOPWORDS\n",
    "    \n",
    "    wordcloud = WordCloud(max_font_size = 60, background_color = 'white', max_words = 250, width = 250, \n",
    "                          stopwords = stopwords, height = 150, scale = 10).generate(newsag) \n",
    "\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(True)\n",
    "    plt.title(cloud_title, fontdict={'fontsize': 18, 'fontweight': 'semibold'}, loc='center', pad=20)\n",
    "    return wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_cl = word_cloud(tass, 'ТАСС')\n",
    "iz_cl = word_cloud(izvestia, 'Известия')\n",
    "kom_cl = word_cloud(kommersant, 'Коммерсант')\n",
    "len_cloud = word_cloud(lenta, 'Лента')\n",
    "full_cloud = word_cloud(full, 'Общее облако')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a system of checkbox widget\n",
    "w1 = widgets.Checkbox(value=False, description ='ТАСС', disabled=False, indent=False)\n",
    "w2 = widgets.Checkbox(value=False, description ='Известия', disabled=False, indent=False)\n",
    "w3 = widgets.Checkbox(value=False, description ='Коммерсант', disabled=False, indent=False)\n",
    "w4 = widgets.Checkbox(value=False, description ='Лента.ru', disabled=False, indent=False)\n",
    "w5 = widgets.Checkbox(value=False, description ='Общее облако', disabled=False, indent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def widgets_res():\n",
    "    \n",
    "    \"\"\"Program takes a result of user widgets input and shows word cloud which was generated earlier\"\"\"\n",
    "    \n",
    "    list_of_check = [w1, w2, w3, w4, w5]\n",
    "    selected = []\n",
    "    for i in list_of_check:\n",
    "        if i.value == True:\n",
    "            selected.append(i)\n",
    "    \n",
    "    for el in selected:\n",
    "        plt.subplots() # to have an opportunity to show several clouds\n",
    "        if el.description == 'ТАСС':\n",
    "            word_cloud(tass, el.description)\n",
    "        if el.description == 'Известия':\n",
    "            word_cloud(izvestia, el.description)\n",
    "        if el.description == 'Коммерсант':\n",
    "            word_cloud(kommersant, el.description)\n",
    "        if el.description == 'Лента.ru':\n",
    "            word_cloud(lenta, el.description)\n",
    "        if el.description == 'Общее облако':\n",
    "            word_cloud(full, el.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def friq_graph():\n",
    "    \n",
    "    \"\"\"The program calculates the frequency of entered by the user word in the text \n",
    "    and total number of words. Then it calculates the ratio of frequency to the total \n",
    "    number of words and creates a graph and a table\"\"\"\n",
    "    \n",
    "    quans = [] # for frequancy of word from input \n",
    "    rel = [] # for ratio\n",
    "    gen = [] # for total\n",
    "    \n",
    "    \n",
    "    # all four loops are similar\n",
    "    tass_split_text = tass.split(' ')\n",
    "    quan_tass = 0\n",
    "    tass_words = 0\n",
    "    for w in tass_split_text:\n",
    "        if w == str(word): \n",
    "            quan_tass = quan_tass + 1\n",
    "        else:\n",
    "            tass_words = tass_words + 1\n",
    "    res1 = quan_tass / (tass_words + quan_tass) # 'plus' to consider all words\n",
    "    quans.append(res1)\n",
    "    rel.append(quan_tass)\n",
    "    gen.append(tass_words+quan_tass)\n",
    "\n",
    "    izvestia_split_text = izvestia.split(' ')\n",
    "    quan_iz = 0\n",
    "    iz_words = 0\n",
    "    for w in izvestia_split_text:\n",
    "        if w == str(word):\n",
    "            quan_iz = quan_iz + 1\n",
    "        else:\n",
    "            iz_words = iz_words + 1\n",
    "    res2 = quan_iz / (iz_words + quan_iz)\n",
    "    quans.append(res2)\n",
    "    rel.append(quan_iz)\n",
    "    gen.append(iz_words+quan_iz)\n",
    "\n",
    "    kommersant_split_text = kommersant.split(' ')\n",
    "    quan_kom = 0\n",
    "    kom_words = 0\n",
    "    for w in kommersant_split_text:\n",
    "        if w == str(word):\n",
    "            quan_kom = quan_kom + 1\n",
    "        else:\n",
    "            kom_words = kom_words + 1\n",
    "    res3 = quan_kom / (kom_words + quan_kom)\n",
    "    quans.append(res3)\n",
    "    rel.append(quan_kom)\n",
    "    gen.append(kom_words+quan_kom)\n",
    "    \n",
    "    lenta_split_text = lenta.split(' ')\n",
    "    quan_len = 0\n",
    "    len_words = 0\n",
    "    for w in lenta_split_text:\n",
    "        if w == str(word):\n",
    "            quan_len = quan_len + 1\n",
    "        else:\n",
    "            len_words = len_words + 1\n",
    "    res4 = quan_len / (len_words + quan_len)\n",
    "    quans.append(res4)\n",
    "    rel.append(quan_len)\n",
    "    gen.append(len_words+quan_len)\n",
    "\n",
    "    lst = [['ТАСС', 'Известия', 'Коммерсант', 'Лента.ru'], quans]\n",
    "    lst2 = [rel, gen]\n",
    "    for_graph = pd.DataFrame(lst).transpose().sort_values(by = 1) # Data Frame for qraph\n",
    "    for_graph.columns = ['СМИ', 'Частота употребления слова']\n",
    "    table = pd.DataFrame(lst2).transpose() # for table\n",
    "    table.columns = ['Абсолютная частота', 'Количество слов']\n",
    "    table.index = ['ТАСС', 'Известия', 'Коммерсант', 'Лента.ru']\n",
    "    graph = sns.barplot(x = for_graph['СМИ'], y = for_graph['Частота употребления слова'], palette = 'GnBu_d')\n",
    "    \n",
    "    return graph, table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Для пользователя"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Здесь вы можете увидеть наглядную повестку дня в виде облаков слов, сгенерированных на основе сегодняшних новостей и заголовков четырех главных российских сетевых СМИ. Выберете один вариант или несколько для сравнения.\n",
    "\n",
    "#### Для этого запустите следующую ячейку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Выберете облака слов, которые вы хотите увидеть:')\n",
    "display(w1, w2, w3, w4, w5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Запустите следующую ячейку, чтобы увидеть результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "widgets_res()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вы можете посмотреть частоту употребления интересующего вас слова в каждом СМИ. Для визуализации используется относительная величена (частота употребления к общему количеству слов). Перед графиком вы увидете таблицу с абсолютными величинами.\n",
    "\n",
    "#### Запустите следующую ячейку и введите интересующее вас слово в появившемся окне"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = input('Введите слово заглавными буквами: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Запутстите следующую ячейку для отображения результата"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "friq_graph()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
